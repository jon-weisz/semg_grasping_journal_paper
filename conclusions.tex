\chapter{Conclusions}
In this thesis, I have presented a series of iterative improvements made to develop the work presented in \cite{eigengrasp} into a practical, integrated system for human-in-the-loop grasping. We began by developing a sampling based approach to improve the robustness of grasps produced by this planner. From a more robust planner, we begin implementing a series of experiments to develop and validate a user interface that can be used with simple interfaces. We have validated the flexibility of this interface using three different input modalities and two different robotic manipulators. Through each iteration of the system, we sought to simplify the user interface while improving the robustness and speed of performance. The results presented in chapters 5 and 6 demonstrate capabilities that have never been shown through similar low throughput, noisy interfaces before, allowing a user to interact with complex scenes relatively quickly using brain-computer interfaces. 

We have validated each stage of our design through experiments with users. Generally, robotics software environments are extremely complex, requiring coordination between a large number of components. It is somewhat normal to encounter software bugs, such as the driver for the Microsoft Kinect hanging indefinitely, on a regular basis. When such bugs happen during an experiment with a subject, the subject tends to lose interest in the experiment, and is likely to stop putting in the effort necessary to learn to use the system effectively. Producing a software environment that is stable enough to bring even a small number of subjects through without encountering adverse events has been one of the most challenging aspects of this research. Additionally, some of these crashes lead to unexpected behaviors that may damage the robots, which are expensive and difficult to repair. Human-in-the-loop robotics experiments that follow an iterative design philosophy are thus relatively rare.

This has only been possible through the integration of a large number of other libraries. The development of this work has coincided with an explosion of open source robotics frameworks, and has benefited enormously from access to the ROS and OpenRAVE frameworks. These software frameworks are used by a much larger population than the custom robotics environments that have often been used in the past, which leads to the identification of many more bugs. Because our system is designed carefully to be modular, we have been able to swap out different components, such as the path planner or robotic manipulator, in order to evaluate many alternatives. Although these frameworks are widely used, it is still a matter of considerable engineering effort to incorporate them into a coherent system. The integrated system presented in this work is the product of multiple researchers that have contributed to this project over the last five years.

The experimental results of the final version of the sEMG driven and EEG driven pipelines demonstrate the flexibility and effectiveness of both the underlying grasp planner and the modular system that we have constructed. This system will form a strong basis on which to begin introducing it to impaired subjects in collaboration with our colleagues at the medical center. The sEMG based pipeline, in particular, shows great promise in its simplicity, and future work may see it integrated into an assistive robotic device outside of the lab.  

\section{Limitations}
This work has addressed only a particular subset of the problems that would be faced by an assistive robotic device, the task of acquiring a stable grasp of an object. There are still many challenges ahead for future research in the areas of navigation and manipulation. The challenge of stable grasping in completely unstructured environments also still remains, particularly in the area of singulating objects that are too close together for planners that do not allow collisions in the grasping process or objects that cannot be clearly seen. 

Chapter 3. \emph{Robust Grasping} presents an approach to improving the robustness of the planned grasps to object localization, but it relies on the $\epsilon_{GWS}$, which makes a large number of assumptions and is only a necessary condition for stability - it is not inherently a \emph{sufficient} condition, which in the general case requires accurate models of the transmission of the fingers. There is no widely accepted generic framework for incorporating these concerns. Additionally, we do not consider the problem of controlling the torques applied to the fingers, as many hands to not provide any interface to control the torques directly. Because of these limitations, the robustness filtering which we have implemented biases the planner strongly in favor of enveloping grasps and power grasps, reducing the flexibility of the grasp planner. We did not explore whether this problem could be rectified. 

Chapter 5. \emph{Using a Minimal Interface Device}  presents a system capable of grasping objects in somewhat cluttered scenes. The object recognition system we have used is only a single view camera that uses only geometric information to recognize objects. When there is significant occlusion of a particular object, this approach will fail. When this happened many times in a row during the experiments, we reshuffled the objects and restarted the trial. Multiple views and contextual information could provide much more robust performance, but development of advanced vision systems is not a focus of this thesis. Additionally, as discussed above, we did not address the problem of singulation. Our system has only been demonstrated on objects that can be grasped without colliding with nearby objects. Singulation is an open area of research, and the state of the art is still too likely to result in damage to the robot to allow experimentation with naive subjects. Finally, in validating the final version of the sEMG pipeline, we did not train the user up to full competency in 2 dimensional control of the cursor controlling the pipeline. Developing a training paradigm that is reproducible is a matter of future research on the part of our collaborators at UC Davis.

Finally, the validation of this system on a larger cohort of impaired users is an important step that is a matter of future research. 

\section{Future Work}
In the time since we published the work presented in this thesis on a sampling based approach to robust grasping, it has been cited by a number of times by authors contrasting the simplicity of the sampling based approach with more mathematically sophisticated or computationally expensive methods (see \cite{kragic-2014} and  \cite{pollard-2013}). However, as of yet no practical, generic method suitable for online planning has been adopted by the grasp planning community. One of the great strengths of this method is that it is purely kinematic, and avoids the complexity and computational cost of dynamic simulation, but this also limits the predictive power of the metric. There is still a great deal of work to be done in this direction.

The planners utilized in this work in both the grasp planning and arm motion planning subtasks are essentially ``any-time'', meaning that they return the best result that they have found thus far after a fixed time interval expires. Recent advances in GPGPU computing have the potential to make such planners orders of magnitude more effective. Up to now, implementing algorithms with the complexity of these planners has been largely impractical, however we are beginning to see some library frameworks such as Bullet3 \url(http://bulletphysics.org/wordpress/) that may make some of the most computationally expensive parts of these planners more tractable. When this happens, whole new families of planning algorithms will become ``real-time'' enough to be integrated in an online planning environment, which may dramatically improve robotic capabilities.

There are two major issues with the current generation of BCI/BMI interface devices that we have explored in this work. The first is the form factor. The Emotiv Epoc and similar headset devices are impractical in their bulk and the expertise needed to keep them working for more than a few minutes. The second is cost - while the Emotiv Epoc system is relatively cheap, we were not impressed with its capabilities. More capable EEG devices have not reached a price point where long term studies with users able to train to use the device over the course of weeks or months have are practical.  This means that all research done using EEG is effectively done on novice users. As cheaper alternatives like the OpenBCI EEG device become available, doing long-term follow up studies to see if users can improve on their ability to use the interface with training is a viable avenue of research that is worth exploring.  

The sEMG system is more practical in both respects than any available EEG device. It is low cost and has a form factor that is much more amenable to long term use. However, in its current implementation it is very much a prototype that is held in place by a combination of tape and silly putty. Integrating this device into pair of glasses or goggles would make them a viable alternative input device that may of interest even beyond the field of robotics as wearable computing devices become more common. This advancement will definitely be necessary before the system can be tried in any long term. We believe that a better form factor combined with a much longer term training paradigm in which the subject is able to use the device in their own home for many weeks may be the key to achieving robust 2D control, and perhaps allowing direct user input into some aspects of the pipeline as though the device were a computer mouse.  

Finally, the user interface needs to be extended to a system suitable for long term autonomous running with many more manipulation capabilities that go beyond acquiring stable grasps. We can easily envision a future in which a robotic assistant can be controlled in an augmented reality system to perform arbitrary tasks, such as washing the dishes, the same way that they are in virtual worlds in video games today. There is nothing in particular standing in the way of extending the system we have built so far to an arbitrary activity of daily living, so long as it can be broken down to a pipeline where a small number of options can demonstrate the user's intent. Exploring which manipulations can and should be implemented first is an open area of research. 

Assistive robotics is still an emerging field. It is not clear what a commercially viable assistive robotic mobile manipulator might look like, as current prototypes are both too expensive and lacking in robustness. As practical devices become available, it will be important to see how we can create user interfaces that enable whatever manipulation capabilities they can provide. As general artificial intelligence seems to still be a long way off, Human-in-the-Loop interfaces will play an essential role in improving the lives of those with limited mobility. Given the capabilities of the interface devices available for such individuals, carefully developed task specific pipelines offer a promising way forward in the near term. 

